import streamlit as st
import pandas as pd
import numpy as np
import re
import tempfile
import networkx as nx
from datetime import timedelta
from pyvis.network import Network
import streamlit.components.v1 as components

# Import visualization libraries (critical!)
import altair as alt  # ‚Üê This was missing ‚Üí caused NameError

# Try to load custom modules (safe fallback)
try:
    from modules.embedding_utils import compute_text_similarity
except ImportError:
    def compute_text_similarity(texts, threshold=0.75, exact=False):
        return []

try:
    from modules.clustering_utils import cluster_texts, build_user_interaction_graph
except ImportError:
    def cluster_texts(texts):
        return [-1] * len(texts)

    def build_user_interaction_graph(df):
        G = nx.Graph()
        sources = df[df['Source'] != 'unknown_user']['Source'].dropna().unique()[:50]
        for src in sources:
            G.add_node(src)
        return G

# -------------------------------
# Utility Functions
# -------------------------------
@st.cache_data
def clean_text(text):
    if pd.isna(text):
        return ""
    return str(text).strip()

def extract_hashtags(text):
    if pd.isna(text):
        return []
    return re.findall(r"#(\w+)", str(text))

# Fix QT @AccountName quoting pattern
def fix_hit_sentence(row):
    opening_text = str(row.get('Opening Text', ''))
    hit_sentence = str(row.get('Hit Sentence', ''))

    if opening_text.startswith('QT') and '@' in opening_text:
        account_name_match = re.match(r'^QT (@[\w]+):', opening_text)
        if account_name_match:
            account_name = account_name_match.group(1)
            if hit_sentence.startswith(f"@{account_name}") and not hit_sentence.startswith('QT'):
                return f"QT {account_name}: {hit_sentence[2:]}"
            elif not hit_sentence.startswith(f"@{account_name}"):
                return f"QT {account_name}: {hit_sentence}"
    return hit_sentence

# -------------------------------
# Standardize DataFrame Function
# -------------------------------
def standardize_dataframe(df, platform_name=None, source_name=None):
    # Normalize column names: strip whitespace and case
    df.columns = [str(col).strip() for col in df.columns]

    # Define mapping (case-insensitive via .lower() check)
    col_map = {
        'Influencer': 'Source',
        'Hit Sentence': 'text',           # Use only Hit Sentence
        'Date': 'Timestamp',
        'createTimeISO': 'Timestamp',
        'authorMeta/name': 'Source',
        'message': 'text',
        'title': 'text',
        'media_name': 'Source',
        'channeltitle': 'Source'
    }

    # Add any variation of URL-like columns
    for col in df.columns:
        if col.lower() in ['url', 'urls', 'link', 'links', 'weburl', 'web_video_url', 'post_url']:
            col_map[col] = 'URL'

    # Also support TikTok's webVideoUrl
    if 'webVideoUrl' in df.columns:
        col_map['webVideoUrl'] = 'URL'

    # Build rename dictionary for existing columns
    rename_dict = {col: std for col, std in col_map.items() if col in df.columns}
    st.write("üîß Using column mapping:", rename_dict)  # Debug

    standardized = {}
    for std_col in ['Source', 'URL', 'Timestamp', 'text']:
        sources = [orig for orig, std in col_map.items() if std == std_col and orig in df.columns]
        if sources:
            combined = df[sources].apply(
                lambda row: next((str(val) for val in row if pd.notna(val)), np.nan), axis=1
            )
            standardized[std_col] = combined
        else:
            standardized[std_col] = np.nan

    df_std = pd.DataFrame(standardized)[['Source', 'URL', 'Timestamp', 'text']].copy()
    df_std['Platform'] = platform_name or source_name or 'Unknown'

    return df_std

# -------------------------------
# Load Default Dataset (GitHub Raw CSV)
# -------------------------------
@st.cache_data(ttl=600)  # Cache for 10 minutes
def load_default_dataset():
    default_url = (
        "https://raw.githubusercontent.com/hanna-tes/CIB-network-monitoring/ "
        "refs/heads/main/Togo_OR_Lome%CC%81_OR_togolais_OR_togolaise_AND_manifest%20-%20Jul%207%2C%202025%20-%205%2012%2053%20PM.csv"
    )
    try:
        df = pd.read_csv(default_url, encoding='utf-16', sep='\t', on_bad_lines='skip', low_memory=False)
        # Clean column names
        df.columns = [str(col).strip() for col in df.columns]
        st.info("‚úÖ Successfully loaded **default X dataset** from GitHub.")
        return df
    except Exception as e:
        st.error(f"‚ùå Failed to load default dataset: {e}")
        return None

# -------------------------------
# Main: Data Input Selection
# -------------------------------
st.set_page_config(page_title="CIB Monitoring Dashboard", layout="wide")
st.title("üïµÔ∏è CIB Monitoring and Analysis Dashboard")

st.sidebar.header("üìÇ Choose Data Source")
data_option = st.sidebar.radio(
    "Select Input Method",
    options=["üìÅ Upload Files", "üåê Use Default Dataset"],
    help="Upload your own data or use the preloaded example."
)

combined_df = pd.DataFrame()

if data_option == "üìÅ Upload Files":
    uploaded_files = st.sidebar.file_uploader(
        "Upload CSV or Excel files", type=['csv', 'xlsx'], accept_multiple_files=True
    )

    if uploaded_files:
        all_dfs = []
        for file in uploaded_files:
            try:
                if file.name.endswith('.csv'):
                    df = pd.read_csv(file, encoding='utf-16', sep='\t', on_bad_lines='skip', low_memory=False)
                else:
                    df = pd.read_excel(file)

                # Clean column names
                df.columns = [str(col).strip() for col in df.columns]

                # Detect platform
                if 'authorMeta/name' in df.columns:
                    platform = 'TikTok'
                elif 'from_name' in df.columns:
                    platform = 'Facebook'
                elif 'channeltitle' in df.columns:
                    platform = 'Telegram'
                elif 'media_name' in df.columns:
                    platform = 'Media'
                else:
                    platform = 'X'

                # Apply X-specific fix
                if platform == 'X' and 'Hit Sentence' in df.columns and 'Opening Text' in df.columns:
                    df['Hit Sentence'] = df.apply(fix_hit_sentence, axis=1)

                # Standardize
                df_std = standardize_dataframe(df, platform_name=platform, source_name=file.name)
                all_dfs.append(df_std)

            except Exception as e:
                st.warning(f"Failed to read {file.name}: {e}")

        if all_dfs:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            st.success(f"‚úÖ Loaded and standardized {len(uploaded_files)} file(s).")

elif data_option == "üåê Use Default Dataset":
    with st.spinner("üì• Loading default dataset..."):
        df = load_default_dataset()
        if df is not None:
            if 'Hit Sentence' in df.columns and 'Opening Text' in df.columns:
                df['Hit Sentence'] = df.apply(fix_hit_sentence, axis=1)
            combined_df = standardize_dataframe(df, platform_name='X', source_name='default_dataset')
            st.success("‚úÖ Default dataset loaded and standardized!")

# -------------------------------
# Post-Processing (All Modes)
# -------------------------------
if not combined_df.empty:
    # Deduplicate
    combined_df.drop_duplicates(inplace=True)

    # Normalize Source
    combined_df['Source'] = combined_df['Source'].astype(str).str.strip().str.lower()
    invalid_sources = ['nan', '<nan>', 'none', 'unknown', '', 'null']
    combined_df.loc[combined_df['Source'].isin(invalid_sources), 'Source'] = 'unknown_user'
    combined_df['Source'] = combined_df['Source'].str.capitalize()

    # Ensure correct order
    combined_df = combined_df[['Source', 'URL', 'Timestamp', 'text', 'Platform']].copy()

    # Parse Timestamp safely
    combined_df['Timestamp'] = pd.to_datetime(
        combined_df['Timestamp'],
        errors='coerce',
        format='ISO8601'  # Efficient and avoids warning
    )

    # Add derived features
    combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)
    combined_df['hashtags'] = combined_df['text'].apply(extract_hashtags)

    # Warn about URL issues
    if combined_df['URL'].isna().all():
        st.warning("‚ö†Ô∏è All URLs are missing. Please check if your data uses a different link column (e.g., Link, post_url).")
    else:
        st.info(f"üîó Found {combined_df['URL'].notna().sum()} valid URLs.")

    # Clustering
    try:
        valid_texts = [t for t in combined_df['cleaned_text'] if t.strip()]
        if valid_texts:
            combined_df['cluster'] = cluster_texts(valid_texts)
        else:
            combined_df['cluster'] = -1
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Clustering failed: {e}")
        combined_df['cluster'] = -1

    # Similarity
    try:
        similar_pairs = compute_text_similarity(
            [t for t in combined_df['cleaned_text']],
            threshold=0.75
        )
        st.session_state['similar_pairs'] = similar_pairs
    except Exception as e:
        st.session_state['similar_pairs'] = []
        st.warning(f"‚ö†Ô∏è Similarity computation failed: {e}")

    # Cache result
    st.session_state['combined_df'] = combined_df

    # Show preview
    st.subheader("üìä Standardized Data Preview (5 Columns)")
    st.dataframe(combined_df.head(10))

    # Export button
    csv = combined_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• Download Cleaned Data",
        data=csv,
        file_name="cib_cleaned_data.csv",
        mime="text/csv"
    )

else:
    st.info("üì§ Please select a data source to begin analysis.")
    st.stop()

# -------------------------------
# Tabs
# -------------------------------
tab1, tab2, tab3 = st.tabs(["üìà Dashboard", "üîç Text Similarity", "‚ÑπÔ∏è About"])

# ========================
# TAB 1: DASHBOARD
# ========================
with tab1:
    st.subheader("1. Summary Statistics")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Total Posts", len(combined_df))
    c2.metric("Unique Sources", combined_df['Source'].nunique())
    c3.metric("Platforms", combined_df['Platform'].nunique())
    c4.metric("Non-null URLs", combined_df['URL'].notna().sum())

    # Platform distribution
    st.markdown("**Platform Distribution:**")
    platform_counts = combined_df['Platform'].value_counts()
    st.bar_chart(platform_counts)

    # --- Posting Timeline ---
    st.subheader("2. Posting Activity Over Time")
    timeline = combined_df.dropna(subset=['Timestamp']).copy()
    timeline['hour'] = timeline['Timestamp'].dt.floor('h')
    post_counts = timeline.groupby('hour').size().reset_index(name='counts')

    chart = alt.Chart(post_counts).mark_line(point=True).encode(
        x=alt.X('hour:T', title='Time'),
        y=alt.Y('counts:Q', title='Number of Posts'),
        tooltip=['hour:T', 'counts']
    ).properties(title="Posting Frequency")
    st.altair_chart(chart, use_container_width=True)

    # --- Trending Hashtags ---
    st.subheader("3. üìà Top Trending Hashtags")
    all_tags = [tag.lower() for tags in combined_df['hashtags'] for tag in tags]
    if all_tags:
        freq = pd.Series(all_tags).value_counts().head(15)
        st.bar_chart(freq)
    else:
        st.info("No hashtags found in the data.")

    # --- High-Risk Accounts ---
    st.subheader("4. üî¥ High-Risk Accounts")
    def compute_risk_scores(df, similar_pairs):
        user_scores = {}
        user_flags = {}

        url_posts = df.dropna(subset=['URL']).sort_values('Timestamp')
        suspicious_urls = []
        for url, group in url_posts.groupby('URL'):
            deltas = group['Timestamp'].diff().dropna()
            rapid_shares = (deltas <= timedelta(minutes=5)).sum()
            if rapid_shares >= 2:
                suspicious_urls.append(url)
        fast_reposters = url_posts[url_posts['URL'].isin(suspicious_urls)]['Source'].value_counts()

        similar_posters = set()
        for i, j, _ in similar_pairs:
            similar_posters.add(df.iloc[i]['Source'])
            similar_posters.add(df.iloc[j]['Source'])

        for src in df['Source'].unique():
            score = 0.0
            flags = []

            count = fast_reposters.get(src, 0)
            if count > 0:
                score += count * 0.3
                flags.append(f"Fast URL ({count})")

            sim_count = sum(1 for p in similar_pairs if df.iloc[p[0]]['Source'] == src or df.iloc[p[1]]['Source'] == src)
            if src in similar_posters:
                score += sim_count * 0.2
                flags.append(f"Similar text ({sim_count})")

            cluster_label = df[df['Source'] == src]['cluster']
            if len(cluster_label) > 1 and cluster_label.mode().iloc[0] != -1:
                size = df['cluster'].value_counts().get(cluster_label.mode().iloc[0], 1)
                if size > 3:
                    score += min(size / 10, 0.5)

            user_scores[src] = round(score, 2)
            user_flags[src] = ", ".join(flags) if flags else "Low activity"

        risk_df = pd.DataFrame({
            'Source': list(user_scores.keys()),
            'Risk Score': list(user_scores.values()),
            'Flags': list(user_flags.values())
        }).sort_values(by='Risk Score', ascending=False)

        return risk_df

    risk_df = compute_risk_scores(combined_df, st.session_state['similar_pairs'])
    high_risk = risk_df[risk_df['Risk Score'] > 0]

    if not high_risk.empty:
        st.dataframe(
            high_risk.style.background_gradient(cmap='Reds', subset=["Risk Score"]),
            use_container_width=True
        )
    else:
        st.info("No high-risk accounts detected.")

    # --- Interactive Network ---
    st.subheader("5. üåê User Interaction Network")
    G = build_user_interaction_graph(combined_df)
    net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black")
    net.from_nx(G)

    risk_dict = risk_df.set_index('Source')['Risk Score'].to_dict()
    for node in net.nodes:
        src = node['label']
        score = risk_dict.get(src, 0)
        hue = int(120 * (1 - min(score, 1)))
        node['color'] = f"hsl({hue}, 80%, 60%)"
        node['title'] = f"<b>{src}</b><br>Risk Score: {score:.2f}"

    net.set_options("""
    var options = {
      "physics": {"enabled": true, "repulsion": {"nodeDistance": 120}},
      "interaction": {"hover": true}
    }
    """)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".html") as tmpfile:
        net.save_graph(tmpfile.name)
        with open(tmpfile.name, 'r', encoding='utf-8') as f:
            components.html(f.read(), height=650)

# ========================
# TAB 2: TEXT SIMILARITY
# ========================
with tab2:
    st.subheader("üîç Detected Similar Text Pairs")
    pairs = st.session_state['similar_pairs']

    if pairs:
        for idx1, idx2, score in sorted(pairs, key=lambda x: -x[2]):
            with st.expander(f"üîÅ Match ‚Ä¢ Score: {score:.3f}"):
                c1, c2 = st.columns(2)
                with c1:
                    st.markdown(f"**Source:** {combined_df.iloc[idx1]['Source']} | **Platform:** {combined_df.iloc[idx1]['Platform']}")
                    st.text_area("Post 1", combined_df.iloc[idx1]['text'], height=120)
                with c2:
                    st.markdown(f"**Source:** {combined_df.iloc[idx2]['Source']} | **Platform:** {combined_df.iloc[idx2]['Platform']}")
                    st.text_area("Post 2", combined_df.iloc[idx2]['text'], height=120)
    else:
        st.info("No similar text pairs found at current threshold.")

# ========================
# TAB 3: ABOUT
# ========================
with tab3:
    st.markdown("""
    ### üïµÔ∏è CIB Monitoring Dashboard

    Detects coordinated inauthentic behavior across:
    - **X (Twitter/Meltwater)**
    - **TikTok**
    - **Facebook**
    - **Telegram**
    - **Media Articles**

    **Features:**
    - üîÑ Auto-standardizes multi-source data
    - üì• Upload or use default dataset
    - üß† Semantic similarity & clustering
    - üìà Hashtag trends
    - üë• Cross-platform network graph
    - üî¥ Risk scoring engine

    Built with: `Streamlit`, `PyVis`, `Altair`, `Pandas`

    üí° Tip: Start with the default dataset to explore!
    """)
